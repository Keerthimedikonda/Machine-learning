{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7tLBWwQGNba",
        "outputId": "78987591-1bbe-4eea-8eef-8234f3f4ab05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [0, 0], Weighted Sum: -0.7\n",
            "Step Function Output: 0\n",
            "Bipolar Step Function Output: -1\n",
            "Sigmoid Function Output: 0.3318122278318339\n",
            "TanH Function Output: -0.6043677771171636\n",
            "ReLU Function Output: 0\n",
            "Leaky ReLU Function Output: -0.006999999999999999\n",
            "\n",
            "Inputs: [0, 1], Weighted Sum: -0.19999999999999996\n",
            "Step Function Output: 0\n",
            "Bipolar Step Function Output: -1\n",
            "Sigmoid Function Output: 0.45016600268752216\n",
            "TanH Function Output: -0.19737532022490398\n",
            "ReLU Function Output: 0\n",
            "Leaky ReLU Function Output: -0.0019999999999999996\n",
            "\n",
            "Inputs: [1, 0], Weighted Sum: 0.030000000000000027\n",
            "Step Function Output: 1\n",
            "Bipolar Step Function Output: 1\n",
            "Sigmoid Function Output: 0.5074994375506203\n",
            "TanH Function Output: 0.029991003238820174\n",
            "ReLU Function Output: 0.030000000000000027\n",
            "Leaky ReLU Function Output: 0.030000000000000027\n",
            "\n",
            "Inputs: [1, 1], Weighted Sum: 0.53\n",
            "Step Function Output: 1\n",
            "Bipolar Step Function Output: 1\n",
            "Sigmoid Function Output: 0.6294831119673949\n",
            "TanH Function Output: 0.4853810906053715\n",
            "ReLU Function Output: 0.53\n",
            "Leaky ReLU Function Output: 0.53\n",
            "\n",
            "Epoch 1\n",
            "Inputs: [0, 0], Weighted Sum: -0.7\n",
            "Step Function Output: 0, Actual Output: 0, Error: 0\n",
            "Updated Weights: [0.73, 0.5], Updated Bias: -0.7\n",
            "\n",
            "Inputs: [0, 1], Weighted Sum: -0.19999999999999996\n",
            "Step Function Output: 0, Actual Output: 0, Error: 0\n",
            "Updated Weights: [0.73, 0.5], Updated Bias: -0.7\n",
            "\n",
            "Inputs: [1, 0], Weighted Sum: 0.030000000000000027\n",
            "Step Function Output: 1, Actual Output: 0, Error: -1\n",
            "Updated Weights: [0.63, 0.5], Updated Bias: -0.7999999999999999\n",
            "\n",
            "Inputs: [1, 1], Weighted Sum: 0.32999999999999996\n",
            "Step Function Output: 1, Actual Output: 1, Error: 0\n",
            "Updated Weights: [0.63, 0.5], Updated Bias: -0.7999999999999999\n",
            "\n",
            "Epoch 2\n",
            "Inputs: [0, 0], Weighted Sum: -0.7999999999999999\n",
            "Step Function Output: 0, Actual Output: 0, Error: 0\n",
            "Updated Weights: [0.63, 0.5], Updated Bias: -0.7999999999999999\n",
            "\n",
            "Inputs: [0, 1], Weighted Sum: -0.29999999999999993\n",
            "Step Function Output: 0, Actual Output: 0, Error: 0\n",
            "Updated Weights: [0.63, 0.5], Updated Bias: -0.7999999999999999\n",
            "\n",
            "Inputs: [1, 0], Weighted Sum: -0.16999999999999993\n",
            "Step Function Output: 0, Actual Output: 0, Error: 0\n",
            "Updated Weights: [0.63, 0.5], Updated Bias: -0.7999999999999999\n",
            "\n",
            "Inputs: [1, 1], Weighted Sum: 0.32999999999999996\n",
            "Step Function Output: 1, Actual Output: 1, Error: 0\n",
            "Updated Weights: [0.63, 0.5], Updated Bias: -0.7999999999999999\n",
            "\n",
            "Training converged!\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "def weighted_sum_calculation(inputs, weights):\n",
        "    #Calculates the weighted sum of inputs as output=x1*w1+x2*w2+x3*w3.....+xi*wi\n",
        "    return sum(x * w for x, w in zip(inputs, weights))\n",
        "\n",
        "def step_function(weighted_sum):\n",
        "    threshold=0\n",
        "    #Step activation function, 1 if greater than or equal to zero else 0\n",
        "    if weighted_sum >= threshold:\n",
        "       return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "def bipolar_step_function(weighted_sum):\n",
        "    threshold=0\n",
        "    #Bipolar step activation function, returns 1 if weighted sum is greater than or equal to thresold else -1\n",
        "    if weighted_sum >= threshold:\n",
        "      return 1\n",
        "    else:\n",
        "      return -1\n",
        "\n",
        "def sigmoid_function(weighted_sum):\n",
        "    #Sigmoid activation function, returns 1/(1+e^-weightes sum)\n",
        "    return 1 / (1 + math.exp(-weighted_sum))\n",
        "\n",
        "def tanh_function(weighted_sum):\n",
        "    #TanH activation function, returns tanh(x)=(e^x-e^-x)/(e^x+e^-x)\n",
        "    return math.tanh(weighted_sum)\n",
        "\n",
        "def relu_function(weighted_sum):\n",
        "    #ReLU activation function, returns weighted sum if weighted sum is > 0 else 0\n",
        "    return max(0, weighted_sum)\n",
        "\n",
        "def leaky_relu_function(weighted_sum):\n",
        "    alpha=0.01\n",
        "    #Leaky ReLU activation function, returns weighted sum if weighted sum is > 0 else aplha*weight sum\n",
        "    if weighted_sum > 0 :\n",
        "       return weighted_sum\n",
        "    else:\n",
        "       return alpha * weighted_sum\n",
        "def error_calculation(predicted, actual):\n",
        "    #Calculates the error between predicted and actual values\n",
        "    return actual - predicted\n",
        "\n",
        "def update_weights(weights, inputs, learning_rate, error):\n",
        "    # Updates weights based on the error and learning rate\n",
        "    return [w + learning_rate * error * x for w, x in zip(weights, inputs)]\n",
        "\n",
        "def update_bias(bias, learning_rate, error):\n",
        "    # Updates the bias term\n",
        "    return bias + learning_rate * error\n",
        "\n",
        "def main():\n",
        "    #initial weights\n",
        "    weights = [0.73, 0.5]\n",
        "    # Bias to achieve AND gate behavior\n",
        "    bias = -0.7\n",
        "    # AND gate (list of tuples)\n",
        "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "    #outputs for AND gate\n",
        "    actual_outputs = [0, 0, 0, 1]\n",
        "    #learning rate\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    for idx, (input1, input2) in enumerate(inputs):\n",
        "        input_values = [input1, input2]\n",
        "        # weighted sum\n",
        "        weighted_sum=weighted_sum_calculation(input_values, weights) + bias\n",
        "        print(f\"Inputs: {input_values}, Weighted Sum: {weighted_sum}\")\n",
        "\n",
        "        # Activation Function results output\n",
        "        step_output = step_function(weighted_sum)\n",
        "        print(f\"Step Function Output: {step_output}\")\n",
        "\n",
        "        # Bipolar step activation function results output\n",
        "        bipolar_step_output = bipolar_step_function(weighted_sum)\n",
        "        print(f\"Bipolar Step Function Output: {bipolar_step_output}\")\n",
        "\n",
        "        #Sigmoid activation function results output\n",
        "        sigmoid_output = sigmoid_function(weighted_sum)\n",
        "        print(f\"Sigmoid Function Output: {sigmoid_output}\")\n",
        "\n",
        "        #TanH activation function results output\n",
        "        tanh_output = tanh_function(weighted_sum)\n",
        "        print(f\"TanH Function Output: {tanh_output}\")\n",
        "\n",
        "        #ReLU activation function results output\n",
        "        relu_output = relu_function(weighted_sum)\n",
        "        print(f\"ReLU Function Output: {relu_output}\")\n",
        "\n",
        "        #Leaky ReLU activation function results output\n",
        "        leaky_relu_output = leaky_relu_function(weighted_sum)\n",
        "        print(f\"Leaky ReLU Function Output: {leaky_relu_output}\\n\")\n",
        "\n",
        "\n",
        "    actual_output = 0\n",
        "    #max epochs\n",
        "    max_epochs = 1000\n",
        "    for epoch in range(max_epochs):\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        total_error = 0\n",
        "\n",
        "        #training the preceptron\n",
        "        for idx, (input1, input2) in enumerate(inputs):\n",
        "            input_values = [input1, input2]\n",
        "            #weighted sum\n",
        "            weighted_sum = weighted_sum_calculation(input_values, weights) + bias\n",
        "            #predicted output\n",
        "            step_output = step_function(weighted_sum)\n",
        "            actual_output = actual_outputs[idx]\n",
        "            error = error_calculation(step_output, actual_output)\n",
        "            total_error += abs(error)\n",
        "            #updating weights and bias\n",
        "            weights = update_weights(weights, input_values, learning_rate, error)\n",
        "            bias = update_bias(bias, learning_rate, error)\n",
        "            #printing\n",
        "            print(f\"Inputs: {input_values}, Weighted Sum: {weighted_sum}\")\n",
        "            print(f\"Step Function Output: {step_output}, Actual Output: {actual_output}, Error: {error}\")\n",
        "            print(f\"Updated Weights: {weights}, Updated Bias: {bias}\\n\")\n",
        "\n",
        "        # Convergence check\n",
        "        if total_error == 0:\n",
        "            print(\"Training converged!\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}